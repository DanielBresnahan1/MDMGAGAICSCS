<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.3"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Ag-AI: README</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Ag-AI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.3 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">README </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p >#Ag-AI #Capstone Project The goal of this project is to build trust in farmers on the idea of artificial intelligence. We plan to do this by allowing the user to interact with an active learning system, providing information to a machine learning algorithm and seeing how their input changes the accuracy of the algorithm. The CapstoneMain is used to run this project. Images folder is used to hold all images. Images must be separated by classification by adding a folder that holds all images of the classification. There must be a path to each file in master to run this program. The user will interact with a web-based user interface, where they’ll be asked to label pictures of corn as either healthy or unhealthy. &ndash;Random Forest Retraining&ndash; To retrain the random forest algorithm, first create a random forest model by classifying images presented by webapp. Once enough confidence is reached, the results screen for the model will show. Scroll down until one is able to see the test images the model classified itself, not the training set provided by the user. Use checkboxes to disagree with pictures labeled by model, then scroll down to the bottom of the page and select the "retrain" button. &ndash;Image Patching&ndash; To patch the images to create useful data for the Neural Network, you need to use the ImagePatching.py File. This file contains a class that will handle all the logic and implementation for image striding, and tiling. To utilize, simply instantiate the class with a few important parameters 1. The directory you wish to save the new files. 2. The size of the patches to generate (recommend 224x224) and 3. The size of the original image. With the class instantiated, call the class method patch (patcher.patch) and supply the location of the image, and the coordinates of the lesion in the form (x1, y2, x2, y2). If the image is negative (ie. does not contain a lesion) supply (0,0,0,0) instead. It is possible to use collate data if you wish to rerun the experiment entirely, however, do note it is a hard coded script, so you will have to change each path name to represent the associated path on your system.</p>
<p >#Release Notes Milestone 2: –Random Forest Saving– Ahead of schedule, we implemented the saving of user training of the random forest algorithm, and added additional necessary functionality to the app to back the saving, such as the reset button. &ndash;Man versus Machine&ndash; Play against a dummy AI by clicking the second button on the home screen. User is then tasked with classifying a series of corn images. Finally, the user is taken to a results page to view the statistics and the machine's selections.</p>
<p >–Professional Neural Network Implementation of Model A is complete, which you can view in the file ModelA.py. To train simply run the script, as the implementation is located within “if <b>name</b>==”__main__”:”</p>
<p >–Split generation The file createSplits is responsible for organizing the total data into 70% train 15% validate and 15% test. To call, make sure your directory is structure as →images_handheld →Train →Test →Validate handheld_annotations.csv And then run the file.</p>
<p >–Batches Iterator batches_iterator.py (and batches_iterator_half_no_lesion.py, but that is broken as of yet) is responsible for handling all batch processing for training the neural networks. It will select n/2 number of positive and n/2 number of negative samples to train the neural network. Additionally, the file will correctly parse and format the image data into numpy arrays of size (3,224,224). When the iterator runs out of images to push for training, it will reshuffle the image list and continue. To use, simply importBatchesIterator from <a class="el" href="namespacebatches__iterator.html">batches_iterator</a>, and instantiate the class with parameters for batch size and directory path. Then, call next() on the saved object. <br  />
</p>
<p >–3D Visualization This class will be added to the webpage at a later date but currently if you manually run the python script you can see a demo of the Visualization. The user will interact with a web-based user interface, where they’ll be asked to label pictures of corn as either healthy or unhealthy.</p>
<p >&ndash;Random Forest Retraining&ndash; To retrain the random forest algorithm, first create a random forest model by classifying images presented by webapp. Once enough confidence is reached, results screen for the model will show. Scroll down until one is able to see the test images the model classified itself, not the training set provided by the user. Use checkboxes to disagree with pictures labeled by model, then scroll down to bottom of page and select "retrain" button.</p>
<p >&ndash;Image Patching&ndash; To patch the images to create useful data for the Neural Network, you need to use the ImagePatching.py File. This file contains a class that will handle all the logic and implementation for iamge striding, and tiling. To utilize, simply instantiate the class with a few important parameters 1. The directory you with to save the new files. 2. The size of the patches to generate (reccomend 224x224) and 3. The size of the original image.</p>
<p >With the class instantiated, call the class method patch (patcher.patch) and supply the location of the image, and the coordinates of the lession in the form (x1, y2, x2, y2). If the image is negative (ie. does not contain a lesion) supply (0,0,0,0) instead.</p>
<p >It is possible to use colate data if you wish to rerun the experiment entirely, however, do note it is a hard coded script, so you will have to change each path name to represent the associated path on your system.</p>
<p >#Release Notes Milestone M4</p>
<p >&ndash;Updated graph functionality&ndash; Graph now shows the correct heat map images when a point is hovered. It will show the x, y and z coordinates of the point hovered as well as a heat map corresponding to each point below the graph based on what images were shown to the user over the course of the Man vs machine trials</p>
<p >&ndash;ML: Created and trained Model T for stage 3 classification. Model T uses a ResNet 50 like Structure with 25 million parameters. The classified heat maps are then fed into stage 4 for final classification. Stage 4 classification is handled by the Linear Classifier Model and uses a random forest classifier to determine what the 3 coordinates final class labeel is. Additionally, testing was done using the test folder and results were saved in a csv file, which was uploaded to the s3 bucket.</p>
<p >&ndash;Man Versus Machine M4&ndash; Functionality added to demonstrate the capabilities of a trained AI against a regular human to classify disease in corn plants. User can now play against the AI by selecting 'Healthy' or 'Unhealthy' 10 times and then view the resulting scores and true images labels at the end. Implemented through logic in web.py and realized by using a different image url for the new s3 server along with updated CSS.</p>
<p >&ndash;CSS M4&ndash; This milestone features an overall resign for the website in terms of visuals, including CSS, boostrap templating, html layout, and javascript. With less css files and better templates, the application should be more maintainable for future development. Hopefully this version is more appealing to look at. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.3
</small></address>
</body>
</html>
